{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN1OxJ/tU7wymuzOUZbA9b4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":28,"metadata":{"id":"xUkZn01uWKsH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756281284848,"user_tz":-120,"elapsed":1214,"user":{"displayName":"Philippe Helluy","userId":"07790988964707423334"}},"outputId":"6eb43834-2d06-448e-999a-090adf9794c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/DLAA_2025/qwen2.5-0.5b-instruct-lora-output\n","\u001b[0m\u001b[01;34mcheckpoint-20\u001b[0m/  evaluation_avant.json  evaluation_checkpoint-20.json  \u001b[01;34mplotdiag\u001b[0m/\n","/content/drive/My Drive/DLAA_2025/qwen2.5-0.5b-instruct-lora-output\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/My Drive/DLAA_2025/qwen2.5-0.5b-instruct-lora-output\"\n","%ls\n","!pwd"]},{"cell_type":"code","source":["import json\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import euclidean_distances\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import os\n","import sys\n","\n","import numpy as np\n","\n","# Add parent directory to sys.path\n","print(os.path.abspath)\n","sys.path.append(os.path.abspath(\"..\"))\n","from ts_dataset import ask_noimage"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Vh17MEVXST8","executionInfo":{"status":"ok","timestamp":1756281296534,"user_tz":-120,"elapsed":46,"user":{"displayName":"Philippe Helluy","userId":"07790988964707423334"}},"outputId":"cf2b6b65-aef1-4e6d-8a9c-43f82e3cdad4"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["<function abspath at 0x7b05a84a20c0>\n"]}]},{"cell_type":"code","source":["# Create the directory if it does not exist\n","output_dir = 'plotdiag'\n","os.makedirs(output_dir, exist_ok=True)"],"metadata":{"id":"XsDTLzEeZRwo","executionInfo":{"status":"ok","timestamp":1756281299288,"user_tz":-120,"elapsed":5,"user":{"displayName":"Philippe Helluy","userId":"07790988964707423334"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Load sentence transformer model\n","model_name = \"paraphrase-MiniLM-L6-v2\"\n","print(\"Loading model \" + model_name)\n","model_st = SentenceTransformer(model_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7AJ_yq7raSsK","executionInfo":{"status":"ok","timestamp":1756281302820,"user_tz":-120,"elapsed":1408,"user":{"displayName":"Philippe Helluy","userId":"07790988964707423334"}},"outputId":"15cbc14b-a00e-43e2-b97c-edae4be45093"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model paraphrase-MiniLM-L6-v2\n"]}]},{"cell_type":"code","source":["# Load a model trained for contradiction detection (NLI)\n","nli_model_name = \"roberta-large-mnli\"\n","print(\"Loading NLI model \" + nli_model_name)\n","nli_tokenizer = AutoTokenizer.from_pretrained(nli_model_name)\n","nli_model = AutoModelForSequenceClassification.from_pretrained(nli_model_name)\n","\n","def compute_semantic_similarity(gold_output, generated_output):\n","    emb_generated = model_st.encode([generated_output], convert_to_tensor=True)\n","    emb_gold = model_st.encode([gold_output], convert_to_tensor=True)\n","\n","    cosine_score = torch.nn.functional.cosine_similarity(emb_generated, emb_gold).cpu().item()\n","    euclidean_score = euclidean_distances(emb_generated.cpu(), emb_gold.cpu())[0][0]\n","\n","    return cosine_score, euclidean_score\n","\n","def detect_contradiction_nli(premise, hypothesis):\n","    inputs = nli_tokenizer.encode_plus(premise, hypothesis, return_tensors='pt', truncation=True)\n","    with torch.no_grad():\n","        logits = nli_model(**inputs).logits\n","    probs = F.softmax(logits, dim=1)\n","    #labels = ['entailment', 'neutral', 'contradiction']\n","    labels = ['no', 'bof', 'ok']\n","    max_idx = torch.argmax(probs).item()\n","    return labels[max_idx], probs[0][max_idx].item()\n","\n","\n","# test the detection of contradiction\n","premise = \"The cat is in the tree.\"\n","hypothesis = \"The moggy is swimming.\"\n","label, score = detect_contradiction_nli(premise, hypothesis)\n","print(f\"Label: {label}, Score: {score}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zuwqJnYYaYSp","executionInfo":{"status":"ok","timestamp":1756281305982,"user_tz":-120,"elapsed":1940,"user":{"displayName":"Philippe Helluy","userId":"07790988964707423334"}},"outputId":"a314ebdd-c4d0-4d07-ad67-0d454e0938a2"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading NLI model roberta-large-mnli\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Label: no, Score: 0.9918491840362549\n"]}]},{"cell_type":"code","source":["#file_name = 'evaluation_checkpoint-20.json'\n","file_name = 'evaluation_avant.json'\n","# Load the specified JSON file\n","with open(file_name, 'r') as f:\n","    evaluation_apres = json.load(f)"],"metadata":{"id":"fkP-_nbga1LZ","executionInfo":{"status":"ok","timestamp":1756281309786,"user_tz":-120,"elapsed":10,"user":{"displayName":"Philippe Helluy","userId":"07790988964707423334"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# recompute the truth\n","def ask_truth(X):\n","    x = np.arange(len(X))\n","    coeffs = np.polyfit(x, X, deg=3)\n","    P = np.poly1d(coeffs)\n","    X_fit = P(x)\n","    # compute the l2 norm of the difference\n","    l2_norm = np.linalg.norm(X - X_fit)/np.sqrt(len(X))\n","    # plt.plot(x,X)\n","    # plt.plot(x,X_fit)\n","    # plt.show()\n","    Pp = P.deriv()\n","    Xp_fit = Pp(x)\n","    delta = Xp_fit[-1] - Xp_fit[0]\n","    # divmin = np.min(Xp_fit)\n","    # divmax = np.max(Xp_fit)\n","    # if divmin > 0:\n","    #     sentence =  \"the time series presents an overall increasing trend\"\n","    # elif divmax < 0:\n","    #     sentence = \"the time series presents an overall decreasing trend\"\n","    # else:\n","    #     sentence = \"the time series presents no uniformly increasing or decreasing trend\"\n","    # if delta > 5:\n","    #     sentence =  \"the time series presents an overall increasing trend\"\n","    # elif delta < -5:\n","    #     sentence = \"the time series presents an overall decreasing trend\"\n","    # else:\n","    #     sentence = \"the time series presents no uniformly increasing or decreasing trend\"\n","    # compute the average of the solution on the 20 first points\n","    average1 = np.mean(X[:20])\n","    # compute the average of the solution on the 20 last points\n","    average2 = np.mean(X[-20:])\n","    if average1 < average2 -3:\n","        sentence_trend = \"the time series shows an overall increasing trend.\"\n","    elif average1 > average2 +3:\n","        sentence_trend = \"the time series shows an overall decreasing trend.\"\n","    else:\n","        sentence_trend = \"the time series shows no uniformly increasing or decreasing trend.\"\n","\n","    print(f\"L2 norm of the difference: {l2_norm}\")\n","    #sentence_noise = self.data_json[i][\"description\"][\"noise\"]\n","    if l2_norm < 2:\n","        sentence_noise = \"the noise intensity is low\"\n","    elif l2_norm > 12:\n","        sentence_noise = \"the noise intensity is high\"\n","    else:\n","        sentence_noise = \"the noise intensity is medium\"\n","    # print(sentence_noise)\n","    # self.data_json[i][\"truth_description\"][\"noise\"] = sentence_noise\n","    # recherche de la localisation en t du maximum et du minimum\n","    pos_max = np.argmax(X)\n","    print('pos_max', pos_max)\n","    if pos_max < 32:\n","        sentence_extrema = \"The maximum is reached around the beginning part of the time series\"\n","    elif pos_max > 96:\n","        sentence_extrema = \"The maximum is reached towards the end of the time series\"\n","    else:\n","        sentence_extrema = \"The maximum is reached around the middle of the time series\"\n","\n","    pos_min = np.argmin(X)\n","    print('pos_min', pos_min)\n","    if pos_min < 32:\n","        sentence_extrema += \" and the minimum is reached around the beginning part of the time series.\"\n","    elif pos_min > 96:\n","        sentence_extrema += \" and the minimum is reached towards the end of the time series.\"\n","    else:\n","        sentence_extrema += \" and the minimum is reached around the middle of the time series.\"\n","\n","    return (sentence_trend, sentence_noise, sentence_extrema)"],"metadata":{"id":"-ZuA-wYXdK83","executionInfo":{"status":"ok","timestamp":1756281313035,"user_tz":-120,"elapsed":15,"user":{"displayName":"Philippe Helluy","userId":"07790988964707423334"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["import ast\n","import re\n","def plot_and_save(i):\n","    input_data = evaluation_apres[i]['input']\n","    series_str = input_data.split('Series: ')[1].strip()\n","    series_str = re.sub(r'\\b0+(\\d)', r'\\1', series_str)\n","    series = ast.literal_eval(series_str)\n","\n","\n","    truth_trend, truth_noise, truth_extrema = ask_truth(series)\n","    print(\"truth_trend: \", truth_trend)\n","    print(\"truth_noise: \", truth_noise)\n","    print(\"truth_extrema: \", truth_extrema)\n","    #series = json.loads(input_data.split('Series: ')[1])\n","    #exit()\n","    gold_output = evaluation_apres[i]['gold_output']\n","    # diagnostic_avant = evaluation_avant[i]['generated_output']\n","    # cute the string to the first 600 characters\n","    # diagnostic_avant = diagnostic_avant[:600]\n","    diagnostic_apres = evaluation_apres[i]['generated_output']\n","    print(\"diagnostic_apres: \", diagnostic_apres)\n","    # in this string keep the string that is between { }\n","    diagnostic_apres = diagnostic_apres[diagnostic_apres.find('{'):diagnostic_apres.rfind('}')+1]\n","    print(\"diagnostic_apres: \", diagnostic_apres)\n","\n","    sentence_gold = []\n","    sentence_after = []\n","\n","    # extract json object from the string before\n","    json_gold = json.loads(gold_output)\n","    sentence_gold.append(json_gold['trend'])\n","    sentence_gold.append(json_gold['noise'])\n","    sentence_gold.append(json_gold['extrema'])\n","\n","    sentence_gold = [truth_trend, truth_noise, truth_extrema]\n","\n","    try:\n","        json_after = json.loads(diagnostic_apres)\n","        print(\"json_after: \", json_after)\n","        sentence_after.append(json_after['trend'])\n","        sentence_after.append(json_after['noise'])\n","        sentence_after.append(json_after['extrema'])\n","        print(\"sentence_after: \", sentence_after)\n","    except (json.JSONDecodeError, KeyError):\n","        sentence_after.extend([\"\"] * 3)\n","\n","    print(\"sentence_after: \", sentence_after)\n","\n","\n","    # Compute similarity scores for each sentence\n","    print(f\"Calculating similarity scores for entry {i}...\")\n","    cosine_score_gold, euclidean_score_gold = [] , []\n","    cosine_score_apres, euclidean_score_apres = [] , []\n","    cass_label_gold, cass_score_gold = [] , []\n","    cass_label_apres, cass_score_apres = [] , []\n","    # for each sentence in the json object\n","    for iss in range(len(sentence_gold)):\n","        # Ensure inputs are strings\n","        after_text = str(sentence_after[iss]) if not isinstance(sentence_after[iss], str) else sentence_after[iss]\n","        gold_text = str(sentence_gold[iss]) if not isinstance(sentence_gold[iss], str) else sentence_gold[iss]\n","\n","        a , b = compute_semantic_similarity(after_text, gold_text)\n","        print(after_text, gold_text, a,b)\n","        cosine_score_apres.append(a)\n","        euclidean_score_apres.append(b)\n","        a,b = detect_contradiction_nli(after_text, gold_text)\n","        cass_label_apres.append(a)\n","        cass_score_apres.append(b)\n","\n","    # Concatenate the similarity scores and CASS labels properly\n","    for cosine, euclidean, cass_label, cass_score in zip(cosine_score_apres, euclidean_score_apres, cass_label_apres, cass_score_apres):\n","        diagnostic_apres += f\"\\n\\nCosine Score: {cosine:.4f}\\nEuclidean Distance: {euclidean:.4f}\"\n","        diagnostic_apres += f\"\\n\\n[CASS] Relation: {cass_label} (score: {cass_score:.4f})\"\n","\n","\n","    fig, axs = plt.subplots(1+3, 1, figsize=(10, 12))\n","    axs[0].set_ylim(0, 99)\n","    axs[0].plot(series)\n","    axs[0].set_xlabel('Time')\n","    axs[0].set_ylabel('Value')\n","    axs[0].set_title('Time Series')\n","\n","\n","    for iss in range(len(sentence_gold)):\n","        axs[1+iss].axis('off')\n","        disp = 'gold: '+str(sentence_gold[iss]) + ' \\n' + 'after: ' + str(sentence_after[iss]) + ' \\n' + 'cosine: ' + str(cosine_score_apres[iss]) + ' \\n' + 'euclidean: ' + str(euclidean_score_apres[iss]) + ' \\n' + 'cass: ' + cass_label_apres[iss] + ' \\n' + 'cass score: ' + str(cass_score_apres[iss])\n","        color = 'red' if cass_label_apres[iss] == 'no' else 'orange' if cass_label_apres[iss] == 'bof' else 'green'\n","        axs[1+iss].text(0.1, 0.5, disp, fontsize=10, verticalalignment='center', wrap=True, color=color)\n","\n","    # color_avant = 'red' if cass_label_gold == 'contradiction' else 'orange' if cass_label_gold == 'neutral' else 'blue'\n","    # color_apres = 'red' if cass_label_apres == 'contradiction' else 'orange' if cass_label_apres == 'neutral' else 'green'\n","\n","    # axs[2].axis('off')\n","    # axs[2].text(0.1, 0.5, diagnostic_avant, fontsize=10, verticalalignment='center', wrap=True, color=color_avant)\n","\n","    # axs[2].axis('off')\n","    # axs[2].text(0.1, 0.5, diagnostic_apres, fontsize=10, verticalalignment='center', wrap=True, color=color_apres)\n","\n","    fig.suptitle(f'Case number {i}', fontsize=16)\n","    file_name = os.path.join(output_dir, f'case_{i}.png')\n","    print(f\"Saving figure to {file_name}\")\n","    fig.savefig(file_name)\n","    plt.close(fig)\n","    scores = [0 if ca == 'no' else 1 if ca == 'ok' else 0.5 for ca in cass_label_apres]\n","    return scores"],"metadata":{"id":"dv-XpXt8bkqR","executionInfo":{"status":"ok","timestamp":1756281316882,"user_tz":-120,"elapsed":6,"user":{"displayName":"Philippe Helluy","userId":"07790988964707423334"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["mean_scores = [0.,0.,0.]\n","for i in range(len(evaluation_apres)):\n","  scores = plot_and_save(i)\n","  print(f\"Scores for case {i}: {scores}\")\n","  for j in range(3):\n","    mean_scores[j] += scores[j]\n","    for j in range(3):\n","      mean_scores[j] /= len(evaluation_apres)\n","      print(f\"Mean scores: {mean_scores}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9A9GOZcb5Q4","executionInfo":{"status":"ok","timestamp":1756281334193,"user_tz":-120,"elapsed":12610,"user":{"displayName":"Philippe Helluy","userId":"07790988964707423334"}},"outputId":"9ef5ce0a-1c4e-4a2f-ac16-da1d715912ef"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["L2 norm of the difference: 8.874099543562421\n","pos_max 68\n","pos_min 0\n","truth_trend:  the time series shows an overall increasing trend.\n","truth_noise:  the noise intensity is medium\n","truth_extrema:  The maximum is reached around the middle of the time series and the minimum is reached around the beginning part of the time series.\n","diagnostic_apres:   \n","``` The series is increasing with an average rate of increase of 0.01 per year.\n","The series has a high level of noise at its beginning and end, with no discernible patterns or trends present.\n","There are two extrema in the series: one near the beginning and another near the end. These extrema represent the highest and lowest values observed in the data set. ```json\n","\n","```json\n","{\n","  \"trend\": \"Increasing\",\n","  \"noise\": \"High\",\n","  \"extrema\": {\n","    \"beginning\": \"near the beginning\",\n","    \"end\": \"near the end\"\n","  }\n","}\n","``` \n","\n","I apologize for the mistake in my previous response. I have corrected it to:\n","\n","- **First sentence:** Increase in value over time.\n","- **Second sentence:** High level of noise due to short-term fluctuations.\n","- **Third sentence:** Approximate localisations of global maximum and minimum within the series.\n","\n","Here's the updated JSON format:\n","```json\n","{\n","  \"trend\": \"Increase in value over time\",\n","  \"noise\": \"High level of noise due to short-term fluctuations\",\n","  \"extrema\": {\n","    \"beginning\": \"near the beginning\",\n","    \"end\": \"near the end\"\n","  }\n","}\n","\n","diagnostic_apres:  {\n","  \"trend\": \"Increasing\",\n","  \"noise\": \"High\",\n","  \"extrema\": {\n","    \"beginning\": \"near the beginning\",\n","    \"end\": \"near the end\"\n","  }\n","}\n","``` \n","\n","I apologize for the mistake in my previous response. I have corrected it to:\n","\n","- **First sentence:** Increase in value over time.\n","- **Second sentence:** High level of noise due to short-term fluctuations.\n","- **Third sentence:** Approximate localisations of global maximum and minimum within the series.\n","\n","Here's the updated JSON format:\n","```json\n","{\n","  \"trend\": \"Increase in value over time\",\n","  \"noise\": \"High level of noise due to short-term fluctuations\",\n","  \"extrema\": {\n","    \"beginning\": \"near the beginning\",\n","    \"end\": \"near the end\"\n","  }\n","}\n","sentence_after:  ['', '', '']\n","Calculating similarity scores for entry 0...\n"," the time series shows an overall increasing trend. 0.09108097851276398 8.332586\n"," the noise intensity is medium 0.09305694699287415 9.024856\n"," The maximum is reached around the middle of the time series and the minimum is reached around the beginning part of the time series. 0.06307943165302277 7.5811048\n","Saving figure to plotdiag/case_0.png\n","Scores for case 0: [0.5, 0.5, 1]\n","Mean scores: [0.1, 0.0, 0.0]\n","Mean scores: [0.1, 0.0, 0.0]\n","Mean scores: [0.1, 0.0, 0.0]\n","Mean scores: [0.02, 0.5, 0.0]\n","Mean scores: [0.02, 0.1, 0.0]\n","Mean scores: [0.02, 0.1, 0.0]\n","Mean scores: [0.004, 0.1, 1.0]\n","Mean scores: [0.004, 0.02, 1.0]\n","Mean scores: [0.004, 0.02, 0.2]\n","L2 norm of the difference: 6.617059319874224\n","pos_max 121\n","pos_min 0\n","truth_trend:  the time series shows an overall increasing trend.\n","truth_noise:  the noise intensity is medium\n","truth_extrema:  The maximum is reached towards the end of the time series and the minimum is reached around the beginning part of the time series.\n","diagnostic_apres:   \n","```\n","\n","```json\n","{\n","  \"trend\": \"Increasing\",\n","  \"noise\": \"Low\",\n","  \"extrema\": {\n","    \"beginning\": \"04\",\n","    \"middle\": \"08\",\n","    \"end\": \"31\"\n","  }\n","}\n","``` \n","\n","This time series has an increasing trend with low noise levels throughout its range, with the highest values at the beginning and end of the data set. The extrema are located in the middle of the data set, indicating the most significant changes or peaks. There is no global maximum or minimum value in this time series, as it spans from the beginning to the end of the dataset without any breaks or abrupt shifts in behavior. The overall pattern suggests that the data represents a continuous increase over time, with occasional fluctuations due to random noise.\n","diagnostic_apres:  {\n","  \"trend\": \"Increasing\",\n","  \"noise\": \"Low\",\n","  \"extrema\": {\n","    \"beginning\": \"04\",\n","    \"middle\": \"08\",\n","    \"end\": \"31\"\n","  }\n","}\n","json_after:  {'trend': 'Increasing', 'noise': 'Low', 'extrema': {'beginning': '04', 'middle': '08', 'end': '31'}}\n","sentence_after:  ['Increasing', 'Low', {'beginning': '04', 'middle': '08', 'end': '31'}]\n","sentence_after:  ['Increasing', 'Low', {'beginning': '04', 'middle': '08', 'end': '31'}]\n","Calculating similarity scores for entry 1...\n","Increasing the time series shows an overall increasing trend. 0.5582243204116821 6.77567\n","Low the noise intensity is medium 0.27924638986587524 8.997813\n","{'beginning': '04', 'middle': '08', 'end': '31'} The maximum is reached towards the end of the time series and the minimum is reached around the beginning part of the time series. 0.20518440008163452 6.7242765\n","Saving figure to plotdiag/case_1.png\n","Scores for case 1: [0.5, 0.5, 1]\n","Mean scores: [0.1008, 0.02, 0.2]\n","Mean scores: [0.1008, 0.004, 0.2]\n","Mean scores: [0.1008, 0.004, 0.04]\n","Mean scores: [0.02016, 0.504, 0.04]\n","Mean scores: [0.02016, 0.1008, 0.04]\n","Mean scores: [0.02016, 0.1008, 0.008]\n","Mean scores: [0.004032, 0.1008, 1.008]\n","Mean scores: [0.004032, 0.02016, 1.008]\n","Mean scores: [0.004032, 0.02016, 0.2016]\n","L2 norm of the difference: 6.595317285135477\n","pos_max 108\n","pos_min 1\n","truth_trend:  the time series shows an overall increasing trend.\n","truth_noise:  the noise intensity is medium\n","truth_extrema:  The maximum is reached towards the end of the time series and the minimum is reached around the beginning part of the time series.\n","diagnostic_apres:   \n","Series name: Stock price\n","Description:\n","Time series: [02, 00, 02, 02, 01, 02, 08, 08, 12, 13, 15, 17, 14, 21, 25, 29, 29, 32, 28, 30, 31, 36, 38, 41, 39, 36, 39, 41, 42, 35, 42, 41, 42, 39, 36, 39, 42, 41, 42, 39, 36, 39, 42, 41, 42, 39, 36, 39, 42, 41, 42, 39, 36, 39, 42, 41, 42, 39, 36, 39, 42,\n","diagnostic_apres:  \n","sentence_after:  ['', '', '']\n","Calculating similarity scores for entry 2...\n"," the time series shows an overall increasing trend. 0.09108097851276398 8.332586\n"," the noise intensity is medium 0.09305694699287415 9.024856\n"," The maximum is reached towards the end of the time series and the minimum is reached around the beginning part of the time series. 0.07334396988153458 7.597459\n","Saving figure to plotdiag/case_2.png\n","Scores for case 2: [0.5, 0.5, 1]\n","Mean scores: [0.1008064, 0.02016, 0.2016]\n","Mean scores: [0.1008064, 0.004032, 0.2016]\n","Mean scores: [0.1008064, 0.004032, 0.04032]\n","Mean scores: [0.02016128, 0.504032, 0.04032]\n","Mean scores: [0.02016128, 0.1008064, 0.04032]\n","Mean scores: [0.02016128, 0.1008064, 0.008064]\n","Mean scores: [0.004032256, 0.1008064, 1.008064]\n","Mean scores: [0.004032256, 0.02016128, 1.008064]\n","Mean scores: [0.004032256, 0.02016128, 0.2016128]\n","L2 norm of the difference: 6.525984691833276\n","pos_max 121\n","pos_min 4\n","truth_trend:  the time series shows an overall increasing trend.\n","truth_noise:  the noise intensity is medium\n","truth_extrema:  The maximum is reached towards the end of the time series and the minimum is reached around the beginning part of the time series.\n","diagnostic_apres:   \n","series = [06, 06, 04, 03, 00, 04, 06, 04, 09, 11, 12, 10, 10, 13, 16, 15, 19, 21, 21, 24, 24, 27, 30, 30, 36, 33, 33, 33, 40, 38, 39, 43, 48, 47, 51, 48, 51, 56, 56, 57, 57, 56, 54, 51, 49, 49, 51, 59, 61, 59, 63, 69, 66, 64, 61, 64, 64, 64, 63, 67, 70, 69, 72, \n","diagnostic_apres:  \n","sentence_after:  ['', '', '']\n","Calculating similarity scores for entry 3...\n"," the time series shows an overall increasing trend. 0.09108097851276398 8.332586\n"," the noise intensity is medium 0.09305694699287415 9.024856\n"," The maximum is reached towards the end of the time series and the minimum is reached around the beginning part of the time series. 0.07334396988153458 7.597459\n","Saving figure to plotdiag/case_3.png\n","Scores for case 3: [0.5, 0.5, 1]\n","Mean scores: [0.10080645120000001, 0.02016128, 0.2016128]\n","Mean scores: [0.10080645120000001, 0.004032256, 0.2016128]\n","Mean scores: [0.10080645120000001, 0.004032256, 0.04032256]\n","Mean scores: [0.02016129024, 0.504032256, 0.04032256]\n","Mean scores: [0.02016129024, 0.10080645120000001, 0.04032256]\n","Mean scores: [0.02016129024, 0.10080645120000001, 0.008064512]\n","Mean scores: [0.004032258048, 0.10080645120000001, 1.008064512]\n","Mean scores: [0.004032258048, 0.02016129024, 1.008064512]\n","Mean scores: [0.004032258048, 0.02016129024, 0.20161290240000002]\n","L2 norm of the difference: 8.145511764902045\n","pos_max 0\n","pos_min 113\n","truth_trend:  the time series shows an overall decreasing trend.\n","truth_noise:  the noise intensity is medium\n","truth_extrema:  The maximum is reached around the beginning part of the time series and the minimum is reached towards the end of the time series.\n","diagnostic_apres:   \n","Series name: 'global_maxima'\n","``` Here is the description in JSON format:\n","\n","```json\n","{\n","  \"trend\": \"Increasing\",\n","  \"noise\": \"High\",\n","  \"extrema\": \"Global Maximum at index 30\"\n","}\n","``` \n","\n","Explanation:\n","- **First sentence**: The series shows an increasing trend as each number increases.\n","- **Second sentence**: There is high noise level present due to extreme values like 99, 96, etc., which disrupts the smooth progression.\n","- **Third sentence**: The global maximum occurs at index 30, indicating the highest value observed within this series. This suggests that there might be significant fluctuations or outliers around this point, possibly indicative of anomalies or changes in the data.\n","diagnostic_apres:  {\n","  \"trend\": \"Increasing\",\n","  \"noise\": \"High\",\n","  \"extrema\": \"Global Maximum at index 30\"\n","}\n","json_after:  {'trend': 'Increasing', 'noise': 'High', 'extrema': 'Global Maximum at index 30'}\n","sentence_after:  ['Increasing', 'High', 'Global Maximum at index 30']\n","sentence_after:  ['Increasing', 'High', 'Global Maximum at index 30']\n","Calculating similarity scores for entry 4...\n","Increasing the time series shows an overall decreasing trend. 0.28437089920043945 8.782955\n","High the noise intensity is medium 0.2679767608642578 9.114046\n","Global Maximum at index 30 The maximum is reached around the beginning part of the time series and the minimum is reached towards the end of the time series. 0.4381013810634613 6.6741724\n","Saving figure to plotdiag/case_4.png\n","Scores for case 4: [0, 0.5, 0.5]\n","Mean scores: [0.0008064516096000001, 0.02016129024, 0.20161290240000002]\n","Mean scores: [0.0008064516096000001, 0.004032258048, 0.20161290240000002]\n","Mean scores: [0.0008064516096000001, 0.004032258048, 0.04032258048]\n","Mean scores: [0.00016129032192, 0.504032258048, 0.04032258048]\n","Mean scores: [0.00016129032192, 0.1008064516096, 0.04032258048]\n","Mean scores: [0.00016129032192, 0.1008064516096, 0.008064516096]\n","Mean scores: [3.2258064384000005e-05, 0.1008064516096, 0.508064516096]\n","Mean scores: [3.2258064384000005e-05, 0.02016129032192, 0.508064516096]\n","Mean scores: [3.2258064384000005e-05, 0.02016129032192, 0.1016129032192]\n"]}]}]}